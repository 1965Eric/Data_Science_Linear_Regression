---
title: "Data Science Linear Regression"
output:
  pdf_document:
    latex_engine: xelatex
  word_document: default
  html_document: default
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The textbook for the Data Science course series is [freely available online](https://rafalab.github.io/dsbook/).

# Learning Objectives

* How linear regression was originally developed by Galton
* What confounding is and how to detect it
* How to examine the relationships between variables by implementing linear regression in R

## Course Overview

There are three major sections in this course: introduction to linear regression, linear models, and confounding.

### Introduction to Linear Regression

In this section, you’ll learn the basics of linear regression through this course’s motivating example, the data-driven approach used to construct baseball teams. You’ll also learn about correlation, the correlation coefficient, stratification, and the variance explained.

### Linear Models

In this section, you’ll learn about linear models. You’ll learn about least squares estimates, multivariate regression, and several useful features of R, such as tibbles, lm, do, and broom. You’ll learn how to apply regression to baseball to build a better offensive metric.

### Confounding

In the final section of the course, you’ll learn about confounding and several reasons that correlation is not the same as causation, such as spurious correlation, outliers, reversing cause and effect, and confounders. You’ll also learn about Simpson’s Paradox.

## Introduction to Regression Overview

In the **Introduction to Regression** section, you will learn the basics of linear regression.

After completing this section, you will be able to:

* Understand how Galton developed **linear regression**.
* Calculate and interpret the **sample correlation**.
* **Stratify** a dataset when appropriate.
* Understand what a **bivariate normal distribution** is.
* Explain what the term **variance explained** means.
* Interpret the two **regression lines**.
    
This section has three parts: **Baseball as a Motivating Example**, **Correlation**, and **Stratification and Variance Explained**.

## Motivating Example: Moneyball

The corresponding section of the textbook is the [case study on Moneyball](https://rafalab.github.io/dsbook/linear-models.html#case-study-moneyball)

**Key points**

Bill James was the originator of the **sabermetrics**, the approach of using data to predict what outcomes best predicted if a team would win.

## Baseball basics

The corresponding section of the textbook is the [section on baseball basics](https://rafalab.github.io/dsbook/linear-models.html#baseball-basics)

**Key points**

* The goal of a baseball game is to score more runs (points) than the other team.
* Each team has 9 batters who have an opportunity to hit a ball with a bat in a predetermined order. 
* Each time a batter has an opportunity to bat, we call it a plate appearance (PA).
* The PA ends with a binary outcome: the batter either makes an out (failure) and returns to the bench or the batter doesn’t (success) and can run around the bases, and potentially score a run (reach all 4 bases).
* We are simplifying a bit, but there are five ways a batter can succeed (not make an out):
1. Bases on balls (BB): the pitcher fails to throw the ball through a predefined area considered to be hittable (the strike zone), so the batter is permitted to go to first base.
2. Single: the batter hits the ball and gets to first base.
3. Double (2B): the batter hits the ball and gets to second base.
4. Triple (3B): the batter hits the ball and gets to third base.
5. Home Run (HR): the batter hits the ball and goes all the way home and scores a run.
* Historically, the batting average has been considered the most important offensive statistic. To define this average, we define a hit (H) and an at bat (AB). Singles, doubles, triples and home runs are hits. The fifth way to be successful, a walk (BB), is not a hit. An AB is the number of times you either get a hit or make an out; BBs are excluded. The batting average is simply H/AB and is considered the main measure of a success rate.

## Bases on Balls or Stolen Bases?

The corresponding section of the textbook is the [base on balls or stolen bases textbook section](https://rafalab.github.io/dsbook/linear-models.html#base-on-balls-or-stolen-bases)

**Key points**

The visualization of choice when exploring the relationship between two variables like home runs and runs is a scatterplot.

*Code: Scatterplot of the relationship between HRs and runs*

```{r}
if(!require(Lahman)) install.packages("Lahman")
if(!require(tidyverse)) install.packages("tidyverse")
if(!require(dslabs)) install.packages("dslabs")

library(Lahman)
library(tidyverse)
library(dslabs)
ds_theme_set()

Teams %>% filter(yearID %in% 1961:2001) %>%
    mutate(HR_per_game = HR / G, R_per_game = R / G) %>%
    ggplot(aes(HR_per_game, R_per_game)) + 
    geom_point(alpha = 0.5)
```

*Code: Scatterplot of the relationship between stolen bases and runs*

```{r}
Teams %>% filter(yearID %in% 1961:2001) %>%
    mutate(SB_per_game = SB / G, R_per_game = R / G) %>%
    ggplot(aes(SB_per_game, R_per_game)) + 
    geom_point(alpha = 0.5)
```

*Code: Scatterplot of the relationship between bases on balls and runs*

```{r}
Teams %>% filter(yearID %in% 1961:2001) %>%
    mutate(BB_per_game = BB / G, R_per_game = R / G) %>%
    ggplot(aes(BB_per_game, R_per_game)) + 
    geom_point(alpha = 0.5)
```

## Assessment - Baseball as a Motivating Example

1. What is the application of statistics and data science to baseball called?

- [ ] A. Moneyball
- [X] B. Sabermetrics
- [ ] C. The “Oakland A’s Approach”
- [ ] D. There is no specific name for this; it’s just data science.

2. Which of the following outcomes is not included in the batting average?

- [ ] A. A home run
- [X] B. A base on balls
- [ ] C. An out
- [ ] D. A single

3. Why do we consider team statistics as well as individual player statistics?

- [X] A. The success of any individual player also depends on the strength of their team.
- [ ] B. Team statistics can be easier to calculate.
- [ ] C. The ultimate goal of sabermetrics is to rank teams, not players.

4. You want to know whether teams with more at-bats per game have more runs per game. What R code below correctly makes a scatter plot for this relationship?

```{r}
Teams %>% filter(yearID %in% 1961:2001 ) %>%
    mutate(AB_per_game = AB/G, R_per_game = R/G) %>%
    ggplot(aes(AB_per_game, R_per_game)) + 
    geom_point(alpha = 0.5)
```

- [ ] A.

```{r, eval=FALSE, echo=TRUE}
Teams %>% filter(yearID %in% 1961:2001 ) %>%
    ggplot(aes(AB, R)) + 
    geom_point(alpha = 0.5)
```

- [X] B.

```{r, eval=FALSE, echo=TRUE}
Teams %>% filter(yearID %in% 1961:2001 ) %>%
    mutate(AB_per_game = AB/G, R_per_game = R/G) %>%
    ggplot(aes(AB_per_game, R_per_game)) + 
    geom_point(alpha = 0.5)
```

- [ ] C.

```{r, eval=FALSE, echo=TRUE}
Teams %>% filter(yearID %in% 1961:2001 ) %>%
    mutate(AB_per_game = AB/G, R_per_game = R/G) %>%
    ggplot(aes(AB_per_game, R_per_game)) + 
    geom_line()
```

- [ ] D.

```{r, eval=FALSE, echo=TRUE}
Teams %>% filter(yearID %in% 1961:2001 ) %>%
    mutate(AB_per_game = AB/G, R_per_game = R/G) %>%
    ggplot(aes(R_per_game, AB_per_game)) + 
    geom_point()
```

5. What does the variable “SOA” stand for in the Teams table?

Hint: make sure to use the help file (```?Teams```).

- [ ] A. sacrifice out
- [ ] B. slides or attempts
- [X] C. strikeouts by pitchers
- [ ] D. accumulated singles

6. Load the **Lahman** library. Filter the ```Teams``` data frame to include years from 1961 to 2001. Make a scatterplot of runs per game versus at bats (```AB```) per game.

```{r}
Teams %>% filter(yearID %in% 1961:2001) %>%
  mutate(AB_per_game = AB / G, R_per_game = R / G) %>%
  ggplot(aes(AB_per_game, R_per_game)) + 
  geom_point(alpha = 0.5)
```

Which of the following is true?

- [ ] A. There is no clear relationship between runs and at bats per game.
- [X] B. As the number of at bats per game increases, the number of runs per game tends to increase.
- [ ] C. As the number of at bats per game increases, the number of runs per game tends to decrease.

7. Use the filtered ```Teams``` data frame from Question 6. Make a scatterplot of win rate (number of wins per game) versus number of fielding errors (```E```) per game.

```{r}
Teams %>% filter(yearID %in% 1961:2001) %>%
  mutate(win_rate = W / G, E_per_game = E / G) %>%
  ggplot(aes(win_rate, E_per_game)) + 
  geom_point(alpha = 0.5)
```

Which of the following is true?

- [ ] A. There is no relationship between win rate and errors per game.
- [ ] B. As the number of errors per game increases, the win rate tends to increase.
- [X] C. As the number of errors per game increases, the win rate tends to decrease.

8. Use the filtered ```Teams``` data frame from Question 6. Make a scatterplot of triples (```X3B```) per game versus doubles (```X2B```) per game.

```{r}
Teams %>% filter(yearID %in% 1961:2001) %>%
  mutate(doubles_per_game = X2B / G, triples_per_game = X3B / G) %>%
  ggplot(aes(doubles_per_game, triples_per_game)) + 
  geom_point(alpha = 0.5)
```

Which of the following is true?

- [X] A. There is no clear relationship between doubles per game and triples per game.
- [ ] B. As the number of doubles per game increases, the number of triples per game tends to increase.
- [ ] C. As the number of doubles per game increases, the number of triples per game tends to decrease.

## Correlation

The corresponding textbook section is [Case Study: is height hereditary?](https://rafalab.github.io/dsbook/regression.html#case-study-is-height-hereditary)

**Key points**

* Galton tried to predict sons' heights based on fathers' heights.
* The mean and standard errors are insufficient for describing an important characteristic of the data: the trend that the taller the father, the taller the son.
* The correlation coefficient is an informative summary of how two variables move together that can be used to predict one variable using the other.

*Code*

```{r}
# create the dataset
if(!require(HistData)) install.packages("HistData")

library(tidyverse)
library(HistData)
data("GaltonFamilies")
set.seed(1983)
galton_heights <- GaltonFamilies %>%
  filter(gender == "male") %>%
  group_by(family) %>%
  sample_n(1) %>%
  ungroup() %>%
  select(father, childHeight) %>%
  rename(son = childHeight)

# means and standard deviations
galton_heights %>%
    summarize(mean(father), sd(father), mean(son), sd(son))

# scatterplot of father and son heights
galton_heights %>%
    ggplot(aes(father, son)) +
    geom_point(alpha = 0.5)
```

## Assessment 4 - Correlation

1. While studying heredity, Francis Galton developed what important statistical concept?

- [ ] A. Standard deviation
- [ ] B. Normal distribution
- [X] C. Correlation
- [ ] D. Probability

2. The correlation coefficient is a summary of what?

- [X] A. The trend between two variables
- [ ] B. The dispersion of a variable
- [ ] C. The central tendency of a variable
- [ ] D. The distribution of a variable

## Assessment 5 - Correlation Coefficient

1. Below is a scatter plot showing the relationship between two variables, x and y.

![Scatter plot](https://user-images.githubusercontent.com/17474099/88270104-6a324080-ccd5-11ea-9cbd-90df12de22bd.png)

From this figure, the correlation between x and y appears to be about:

- [X] A. -0.9
- [ ] B. -0.2
- [ ] C. 0.9
- [ ] D. 2

## Assessment 6 - Sample Correlation is a Random Variable

1. Instead of running a Monte Carlo simulation with a sample size of 25 from our 179 father-son pairs, we now run our simulation with a sample size of 50.

Would you expect the mean of our sample correlation to increase, decrease, or stay approximately the same?

- [ ] A. Increase
- [ ] B. Decrease
- [X] C. Stay approximately the same

2. Instead of running a Monte Carlo simulation with a sample size of 25 from our 179 father-son pairs, we now run our simulation with a sample size of 50.

Would you expect the standard deviation of our sample correlation to increase, decrease, or stay approximately the same?

- [ ] A. Increase
- [X] B. Decrease
- [ ] C. Stay approximately the same

## Assessment 7 - Anscombe’s Quartet/Stratification

1. Look at the figure below. The slope of the regression line in this figure is equal to what, in words?

![Scatter plot of son and father heights with son heights on the y-axis and father heights on the x-axis](https://user-images.githubusercontent.com/17474099/88270632-373c7c80-ccd6-11ea-8dec-eb7315a564b1.png)

- [X] A. Slope = (correlation coefficient of son and father heights) * (standard deviation of sons’ heights / standard deviation of fathers’ heights)
- [ ] B. Slope = (correlation coefficient of son and father heights) * (standard deviation of fathers’ heights / standard deviation of sons’ heights)
- [ ] C. Slope = (correlation coefficient of son and father heights) / (standard deviation of sons’ heights * standard deviation of fathers’ heights)
- [ ] D. Slope = (mean height of fathers) - (correlation coefficient of son and father heights * mean height of sons).

2. Why does the regression line simplify to a line with intercept zero and slope when we standardize our x and y variables? Try the simplification on your own first!

- [ ] A. When we standardize variables, both x and y will have a mean of one and a standard deviation of zero. When you substitute this into the formula for the regression line, the terms cancel out until we have the following equation: $y_i=px_i$.
- [X] B. When we standardize variables, both x and y will have a mean of zero and a standard deviation of one. When you substitute this into the formula for the regression line, the terms cancel out until we have the following equation: $y_i = px_i$.
- [ ] C. When we standardize variables, both x and y will have a mean of zero and a standard deviation of one. When you substitute this into the formula for the regression line, the terms cancel out until we have the following equation: $y_i=px_i$.

3. What is a limitation of calculating conditional means?

- [X] A. Each stratum we condition on (e.g., a specific father’s height) may not have many data points.
- [X] B. Because there are limited data points for each stratum, our average values have large standard errors.
- [X] C. Conditional means are less stable than a regression line.
- [ ] D. Conditional means are a useful theoretical tool but cannot be calculated.

## Assessment 8 - Bivariate Normal Distribution

1. A regression line is the best prediction of Y given we know the value of X when:

- [X] A. X and Y follow a bivariate normal distribution.
- [ ] B. Both X and Y are normally distributed.
- [ ] C. Both X and Y have been standardized.
- [ ] D. There are at least 25 X-Y pairs.

2. Which one of the following scatterplots depicts an x and y distribution that is NOT well-approximated by the bivariate normal distribution?

- [X] A.

![Scatter plot A](https://user-images.githubusercontent.com/17474099/88281437-f64d6380-cce7-11ea-8401-dc487c96fcd8.png)

- [ ] B.

![Scatter plot B](https://user-images.githubusercontent.com/17474099/88281550-2dbc1000-cce8-11ea-8fbc-31d4a9f9baf3.png)

- [ ] C.

![Scatter plot C](https://user-images.githubusercontent.com/17474099/88281645-58a66400-cce8-11ea-9dfa-c39b2114ffa7.png)

- [ ] D.

![Scatter plot D](https://user-images.githubusercontent.com/17474099/88281807-986d4b80-cce8-11ea-9d0d-719572315c01.png)

## Assessment 9 - Variance Explained

1. We previously calculated that the correlation coefficient between fathers’ and sons’ heights is 0.5.

Given this, what percent of the variation in sons’ heights is explained by fathers’ heights?

- [ ] A. 0%
- [X] B. 25%
- [ ] C. 50%
- [ ] D. 75%

## Linear Models Overview

In the Linear Models section, you will learn how to do linear regression.

After completing this section, you will be able to:

* Use multivariate regression to adjust for confounders.
* Write linear models to describe the relationship between two or more variables.
* Calculate the least squares estimates for a regression model using the lm function.
* Understand the differences between tibbles and data frames.
* Use the do function to bridge R functions and the tidyverse.
* Use the tidy, glance, and augment functions from the broom package.
* Apply linear regression to measurement error models.

This section has four parts: Introduction to Linear Models, Least Squares Estimates, Tibbles, do, and broom, and Regression and Baseball.

The textbook for this section is available [here](https://rafalab.github.io/dsbook/linear-models.html#confounding)

## Assessment 1 - Confounding: Are BBs More Predictive?

1. Why is the number of home runs considered a confounder of the relationship between bases on balls and runs per game?

- [ ] A. Home runs is not a confounder of this relationship.
- [ ] B. Home runs are the primary cause of runs per game.
- [ ] C. The correlation between home runs and runs per game is stronger than the correlation between bases on balls and runs per game.
- [X] D. Players who get more bases on balls also tend to have more home runs; in addition, home runs increase the points per game.

## Assessment 2 - Stratification and Multivariate Regression

1. As described in the video, when we stratified our regression lines for runs per game vs. bases on balls by the number of home runs, what happened?

- [X] A. The slope of runs per game vs. bases on balls within each stratum was reduced because we removed confounding by home runs.
- [ ] B. The slope of runs per game vs. bases on balls within each stratum was reduced because there were fewer data points.
- [ ] C. The slope of runs per game vs. bases on balls within each stratum increased after we removed confounding by home runs.
- [ ] D. The slope of runs per game vs. bases on balls within each stratum stayed about the same as the original slope.

## Assessment 3 - Linear Models

1. We run a linear model for sons’ heights vs. fathers’ heights using the Galton height data, and get the following results:

```{r, eval=FALSE, echo=TRUE}
> lm(son ~ father, data = galton_heights)

Call:
lm(formula = son ~ father, data = galton_heights)

Coefficients:
(Intercept)    father  
    35.71       0.50  
```

Interpret the numeric coefficient for “father.”

- [ ] A. For every inch we increase the son’s height, the predicted father’s height increases by 0.5 inches.
- [X] B. For every inch we increase the father’s height, the predicted son’s height grows by 0.5 inches.
- [ ] C. For every inch we increase the father’s height, the predicted son’s height is 0.5 times greater.

2. We want the intercept term for our model to be more interpretable, so we run the same model as before but now we subtract the mean of fathers’ heights from each individual father’s height to create a new variable centered at zero.

```{r, eval=FALSE, echo=TRUE}
galton_heights <- galton_heights %>%
    mutate(father_centered=father - mean(father))
```
We run a linear model using this centered fathers’ height variable.

```{r, eval=FALSE, echo=TRUE}
> lm(son ~ father_centered, data = galton_heights)

Call:
lm(formula = son ~ father_centered, data = galton_heights)

Coefficients:
(Intercept)    father_centered  
    70.45          0.50  
```

Interpret the numeric coefficient for the intercept.

- [X] A. The height of a son of a father of average height is 70.45 inches.
- [ ] B. The height of a son when a father’s height is zero is 70.45 inches.
- [ ] C. The height of an average father is 70.45 inches.

## Assessment 4 - Least Squares Estimates (LSE)

1. The following code was used in the video to plot RSS with $\beta_0=25$.

```{r, eval=FALSE, echo=TRUE}
beta1 = seq(0, 1, len=nrow(galton_heights))
results <- data.frame(beta1 = beta1,
                      rss = sapply(beta1, rss, beta0 = 25))
results %>% ggplot(aes(beta1, rss)) + geom_line() + 
  geom_line(aes(beta1, rss), col=2)
```

In a model for sons’ heights vs fathers’ heights, what is the least squares estimate (LSE) for $\beta_1$ if we assume $\hat{\beta}_{0}$ is 36?

- [ ] A. 0.65
- [X] B. 0.5
- [ ] C. 0.2
- [ ] D. 12

2. The least squares estimates for the parameters $\beta_1,\beta_2...\beta_n$ **minimize** the residual sum of squares.

## Assessment 5 - The lm Function

1. Run a linear model in R predicting the number of runs per game based on the number of bases on balls and the number of home runs. Remember to first limit your data to 1961-2001.

What is the coefficient for bases on balls?

- [X] A. 0.39
- [ ] B. 1.56
- [ ] C. 1.74
- [ ] D. 0.027

## Assessment 6 - LSE are Random Variables

1. We run a Monte Carlo simulation where we repeatedly take samples of N = 100 from the Galton heights data and compute the regression slope coefficients for each sample:

```{r, eval=FALSE, echo=TRUE}
B <- 1000
N <- 100
lse <- replicate(B, {
  sample_n(galton_heights, N, replace = TRUE) %>% 
    lm(son ~ father, data = .) %>% .$coef 
})

lse <- data.frame(beta_0 = lse[1,], beta_1 = lse[2,]) 
```
What does the central limit theorem tell us about the variables beta_0 and beta_1?

- [X] A. They are approximately normally distributed.
- [X] B. The expected value of each is the true value of $\beta_0$ and $\beta_1$ (assuming the Galton heights data is a complete population).
- [ ] C. The central limit theorem does not apply in this situation.
- [ ] D. It allows us to test the hypothesis that $\beta_0=0$ and $\beta_0=1$

2. In an earlier video, we ran the following linear model and looked at a summary of the results.

```{r, eval=FALSE, echo=TRUE}
$\beta_0 $
> mod <- lm(son ~ father, data = galton_heights)
> summary(mod)

Call:
lm(formula = son ~ father, data = galton_heights)

Residuals:
   Min     1Q  Median     3Q    Max 
-5.902  -1.405  0.092    1.342  8.092 

Coefficients:
                 Estimate  Std. Error  t value     Pr(>|t|)  
(Intercept)     35.7125     4.5174       7.91    2.8e-13 ***
father           0.5028     0.0653       7.70    9.5e-13 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
$\beta_0 $
```

What null hypothesis is the second p-value (the one in the father row) testing?

- [ ] A. $\beta_1 = 1$, where $\beta_1$ is the coefficient for the variable “father.”
- [ ] B. $\beta_1 = 0.503$, where $\beta_1$ is the coefficient for the variable “father.”
- [X] C. $\beta_1 = 0$, where $\beta_1$ is the coefficient for the variable "father."

## Assessment 7 - Predicted Variables are Random Variables

1. Which R code(s) below would properly plot the predictions and confidence intervals for our linear model of sons’ heights?

- [ ] A.
```{r, eval=FALSE, echo=TRUE}
galton_heights %>% ggplot(aes(father, son)) +
  geom_point() +
  geom_smooth()
```
- [X] B.
```{r, eval=FALSE, echo=TRUE}
galton_heights %>% ggplot(aes(father, son)) +
  geom_point() +
  geom_smooth(method = "lm")
```
- [X] C.
```{r, eval=FALSE, echo=TRUE}
model <- lm(son ~ father, data = galton_heights)
predictions <- predict(model, interval = c("confidence"), level = 0.95)
data <- as.tibble(predictions) %>% bind_cols(father = galton_heights$father)

ggplot(data, aes(x = father, y = fit)) +
  geom_line(color = "blue", size = 1) + 
  geom_ribbon(aes(ymin=lwr, ymax=upr), alpha=0.2) + 
  geom_point(data = galton_heights, aes(x = father, y = son))
```
- [ ] D.
```{r, eval=FALSE, echo=TRUE}
model <- lm(son ~ father, data = galton_heights)
predictions <- predict(model)
data <- as.tibble(predictions) %>% bind_cols(father = galton_heights$father)

ggplot(data, aes(x = father, y = fit)) +
  geom_line(color = "blue", size = 1) + 
  geom_point(data = galton_heights, aes(x = father, y = son))
```

## Assessment 8 - Advanced dplyr: Tibbles

1. What problem do we encounter when we try to run a linear model on our baseball data, grouping by home runs?

- [ ] A. There is not enough data in some levels to run the model. 
- [X] B. The ```lm``` function does not know how to handle grouped tibbles. 
- [ ] C. The results of the ```lm``` function cannot be put into a tidy format.

2. Tibbles are similar to what other class in R?

- [ ] A. Vectors
- [ ] B. Matrices
- [X] C. Data frames
- [ ] D. Lists

## Assessment 9 - Tibbles: Differences from Data Frames

1. What are some advantages of tibbles compared to data frames?

- [X] A. Tibbles display better.
- [X] B. If you subset a tibble, you always get back a tibble.
- [X] C. Tibbles can have complex entries.
- [X] D. Tibbles can be grouped.

## Assessment 10 - do

1. What are two advantages of the do command, when applied to the tidyverse?

- [ ] A. It is faster than normal functions.
- [ ] B. It returns useful error messages.
- [X] C. It understands grouped tibbles.
- [X] D. It always returns a data.frame.

2. You want to take the tibble dat, which we’ve been using in this video, and run the linear model R ~ BB for each strata of HR. Then you want to add three new columns to your grouped tibble: the coefficient, standard error, and p-value for the BB term in the model.

You’ve already written the function ```get_slope```, shown below.
```{r, eval=FALSE, echo=TRUE}
get_slope <- function(data) {
  fit <- lm(R ~ BB, data = data)
  sum.fit <- summary(fit)

  data.frame(slope = sum.fit$coefficients[2, "Estimate"], 
             se = sum.fit$coefficients[2, "Std. Error"],
             pvalue = sum.fit$coefficients[2, "Pr(>|t|)"])
}
```

What additional code could you write to accomplish your goal?

- [ ] A.
```{r, eval=FALSE, echo=TRUE}
dat %>% 
  group_by(HR) %>% 
  do(get_slope)
```
- [X] B.
```{r, eval=FALSE, echo=TRUE}
dat %>% 
  group_by(HR) %>% 
  do(get_slope(.))
```
- [ ] C.
```{r, eval=FALSE, echo=TRUE}
dat %>% 
  group_by(HR) %>% 
  do(slope = get_slope(.))
```
- [ ] D.
```{r, eval=FALSE, echo=TRUE}
dat %>% 
  do(get_slope(.))
```

## Assessment 11 - broom

1. The output of a broom function is always what?

- [X] A. A data.frame
- [ ] B. A list
- [ ] C. A vector

2. You want to know whether the relationship between home runs and runs per game varies by baseball league. You create the following dataset:
```{r, eval=FALSE, echo=TRUE}
dat <- Teams %>% filter(yearID %in% 1961:2001) %>%
  mutate(HR = HR/G,
         R = R/G) %>%
  select(lgID, HR, BB, R)
```
What code would help you quickly answer this question?

- [X] A.
```{r, eval=FALSE, echo=TRUE}
dat %>% 
  group_by(lgID) %>% 
  do(tidy(lm(R ~ HR, data = .), conf.int = T)) %>% 
  filter(term == "HR")
```
- [ ] B.
```{r, eval=FALSE, echo=TRUE}
dat %>% 
  group_by(lgID) %>% 
  do(glance(lm(R ~ HR, data = .)))
```
- [ ] C.
```{r, eval=FALSE, echo=TRUE}
dat %>% 
  do(tidy(lm(R ~ HR, data = .), conf.int = T)) %>% 
  filter(term == "HR")
```
- [ ] D.
```{r, eval=FALSE, echo=TRUE}
dat %>% 
  group_by(lgID) %>% 
  do(mod = lm(R ~ HR, data = .))
```

## Assessment 12 - Building a Better Offensive Metric for Baseball

1. What is the final linear model we use to predict runs scored per game?

- [ ] A. ```lm(R ~ BB + HR)```
- [ ] B. ```lm(HR ~ BB + singles + doubles + triples)```
- [X] C. ```lm(R ~ BB + singles + doubles + triples + HR)```
- [ ] D. ```lm(R ~ singles + doubles + triples + HR)```

2. We want to estimate runs per game scored by individual players, not just by teams. What summary metric do we calculate to help estimate this?

Look at the code from the video for a hint:
```{r, eval=FALSE, echo=TRUE}
pa_per_game <- Batting %>% 
  filter(yearID == 2002) %>% 
  group_by(teamID) %>%
  summarize(pa_per_game = sum(AB+BB)/max(G)) %>% 
  .$pa_per_game %>% 
  mean
```

- [ ] A. ```pa_per_game```: the mean number of plate appearances per team per game for each team
- [ ] B. ```pa_per_game```: the mean number of plate appearances per game for each player
- [X] C. ```pa_per_game```: the number of plate appearances per team per game, averaged across all teams

3. Imagine you have two teams. Team A is comprised of batters who, on average, get two bases on balls, four singles, one double, and one home run. Team B is comprised of batters who, on average, get one base on balls, six singles, two doubles, and one triple.

Which team scores more runs, as predicted by our model?

- [ ] A. Team A
- [X] B. Team B
- [ ] C. Tie
- [ ] D. Impossible to know

## Assessment 13 - On Base Plus Slugging (OPS)

1. The on-base-percentage plus slugging percentage (OPS) metric gives the most weight to:

- [ ] A. Singles
- [ ] B. Doubles
- [ ] C. Triples
- [X] D. Home Runs

## Assessment 14 - Regression Fallacy

1. What statistical concept properly explains the “sophomore slump”?

- [X] A. Regression to the mean
- [ ] B. Law of averages
- [ ] C. Normal distribution

## Assessment 15 - Measurement Error Models

1. In our model of time vs. observed_distance, the randomness of our data was due to:

- [ ] A. sampling 
- [ ] B. natural variability 
- [X] C. measurement error

2. Which of the following are important assumptions about the measurement errors in this experiment?

- [X] A. The measurement error is random
- [X] B. The measurement error is independent
- [X] C. The measurement error has the same distribution for each time i

3. Which of the following scenarios would violate an assumption of our measurement error model?

- [ ] A. The experiment was conducted on the moon. 
- [X] B. There was one position where it was particularly difficult to see the dropped ball. 
- [ ] C. The experiment was only repeated 10 times, not 100 times.

## Confounding Overview

In the Confounding section, you will learn what is perhaps the most important lesson of statistics: that correlation is not causation.

After completing this section, you will be able to:

* Identify examples of spurious correlation and explain how data dredging can lead to spurious correlation.
* Explain how outliers can drive correlation and learn to adjust for outliers using Spearman correlation.
* Explain how reversing cause and effect can lead to associations being confused with causation.
* Understand how confounders can lead to the misinterpretation of associations.
* Explain and give examples of Simpson’s Paradox.

This section has one part: Correlation is Not Causation.

The textbook for this section is available [here](https://rafalab.github.io/dsbook/association-is-not-causation.html)

## Assessment 1 - Correlation is Not Causation: Spurious Correlation

1. In the video, we ran one million tests of correlation for two random variables, X and Y.

How many of these correlations would you expect to have a significant p-value (p>0.05), just by chance?

- [ ] A. 5,000
- [X] B. 50,000
- [ ] C. 100,000
- [ ] D. It’s impossible to know

2. Which of the following are examples of p-hacking?

- [X] A. Looking for associations between an outcome and several exposures and only reporting the one that is significant.
- [X] B. Trying several different models and selecting the one that yields the smallest p-value.
- [X] C. Repeating an experiment multiple times and only reporting the one with the smallest p-value.
- [ ] D. Using a Monte Carlo simulations in an analysis.

## Assessment 2 - Correlation is Not Causation: Outliers

1. The Spearman correlation coefficient is robust to outliers because:

- [ ] A. It drops outliers before calculating correlation.
- [ ] B. It is the correlation of standardized values.
- [X] C. It calculates correlation between ranks, not values.

## Assessment 3 - Correlation is Not Causation: Reversing Cause and Effect

1. Which of the following may be examples of reversed cause and effect?

- [X] A. Past smokers who have quit smoking may be more likely to die from lung cancer.
- [ ] B. Tall fathers are more likely to have tall sons. 
- [X] C. People with high blood pressure tend to have a healthier diet.
- [X] D. Individuals in a low social status have a higher risk of schizophrenia.

## Assessment 4 - Correlation is Not Causation: Confounders

1. What can you do to determine if you are misinterpreting results because of a confounder?

- [ ] A. Nothing, if the p-value says the result is significant, then it is.
- [X] B. More closely examine the results by stratifying and plotting the data.
- [ ] C. Always assume that you are misinterpreting the results.
- [ ] D. Use linear models to tease out a confounder.

2. Look again at the admissions data using ?admissions. What important characteristic of the table variables do you need to know to understand the calculations used in this video? Select the best answer.

- [ ] A. The data is from 1973.
- [ ] B. The columns “major” and “gender” are of class character, while “admitted” and “applicants” are numeric.
- [ ] C. The data is from the “dslabs” package.
- [X] D. The column “admitted” is the percent of student admitted, while the column “applicants” is the total number of applicants.

3. In the example in the video, major selectivity confounds the relationship between UC Berkley admission rates and gender because:

- [ ] A. It was harder for women to be admitted to UC Berkeley.
- [X] B. Major selectivity is associated with both admission rates and with gender, as women tended to apply to more selective majors.
- [ ] C. Some majors are more selective than others
- [ ] D. Major selectivity is not a confounder.

## Assessment 5 - Simpson’s Paradox

1. Admission rates at UC Berkeley are an example of Simpson’s Paradox because:

- [X] A. It appears that men have higher a higher admission rate than women, however, after we stratify by major, we see that on average women have a higher admission rate than men.
- [ ] B. It was a paradox that women were being admitted at a lower rate than men.
- [ ] C. The relationship between admissions and gender is confounded by major selectivity.
